{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_OTuVwGdlZb"
   },
   "source": [
    "# Install the <b>simpletransformers</b> library (restart runtime if using Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svum1ZL4dlHn"
   },
   "outputs": [],
   "source": [
    "! pip install simpletransformers\n",
    "! pip install tokenizers==0.9.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WsJ66z-hpl0"
   },
   "source": [
    "# Download the datasets(files converted to xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXmiy2SAiO3-"
   },
   "outputs": [],
   "source": [
    "## Tamil\n",
    "! gdown https://drive.google.com/uc?id=10pPg_WI0Qzgvi-qwxcyWbtfqYda0DsoM\n",
    "! gdown https://drive.google.com/uc?id=1iF4sZ1XFL4pG6YVGrPWEsN1NMiPDQ0cA\n",
    "! gdown https://drive.google.com/uc?id=1CgoMCL-ZKda6G8xfrVeNPPiSt2S6hBio\n",
    "\n",
    "\n",
    "## Malayalam\n",
    "! gdown https://drive.google.com/uc?id=1aA-cxg_iRtM83NgCDSluBIL7fsNUiJg_\n",
    "! gdown https://drive.google.com/uc?id=1y50Xnd685oCoziVytpVtBcJ8CZizE5Nt\n",
    "! gdown https://drive.google.com/uc?id=1h7vrLgccRuEanpDipHMFv2q4ptX65HlW\n",
    "\n",
    "\n",
    "## Kannada\n",
    "! gdown https://drive.google.com/uc?id=1k6on-7xMJ6zyaFpCxrV3CZii4y1UD4KP\n",
    "! gdown https://drive.google.com/uc?id=1Dx-TByQ2gIjvHmmNJTA-Aj8rqwGjHo7i\n",
    "! gdown https://drive.google.com/uc?id=1zG-K2hdpx4n-Geqpww7s5bd8CYCgeJBH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEnBdaKXLHTc"
   },
   "source": [
    "# Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfLU9WOp-_eA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, matthews_corrcoef\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import os\n",
    "import tarfile\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbnLw7rfLOBg"
   },
   "source": [
    "# Choose language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c01UdHpuvmG"
   },
   "outputs": [],
   "source": [
    "lang = input('Choose language: 1 for tamil, 2 for malayalam, 3 for kannada: ')\n",
    "\n",
    "class_list = ['Not_offensive',\n",
    " 'Offensive_Targeted_Insult_Group',\n",
    " 'Offensive_Targeted_Insult_Individual',\n",
    " 'Offensive_Targeted_Insult_Other',\n",
    " 'Offensive_Untargetede']\n",
    "\n",
    "\n",
    "train_file_name = None\n",
    "dev_file_name = None\n",
    "\n",
    "if lang == '1': \n",
    "    lang = 'Tamil'\n",
    "    class_list.append('not-Tamil')\n",
    "    train_file_name = 'tamil_offensive_full_train.xlsx'\n",
    "    dev_file_name = 'tamil_offensive_full_dev.xlsx'\n",
    "    test_file_name = 'tamil_offensive_full_test_with_labels.xlsx'\n",
    "elif lang == '2': \n",
    "    lang = 'malayalam'\n",
    "    # As it contains only 5 classes ('Offensive_Targeted_Insult_Other' is not present)\n",
    "    class_list.append('not-malayalam')\n",
    "    class_list.remove('Offensive_Targeted_Insult_Other')\n",
    "\n",
    "    train_file_name = 'mal_full_offensive_train.xlsx'\n",
    "    dev_file_name = 'mal_full_offensive_dev.xlsx'\n",
    "    test_file_name = 'mal_full_offensive_test_with_labels.xlsx'\n",
    "elif lang == '3': \n",
    "    lang = 'Kannada'\n",
    "    class_list.append('not-Kannada')\n",
    "    train_file_name = 'kannada_offensive_train.xlsx'\n",
    "    dev_file_name = 'kannada_offensive_dev.xlsx'(test_df['Input'].apply(lambda val: str(val))\n",
    "    test_file_name = 'kannada_offensive_test_with_labels.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OsZM7irsiDr"
   },
   "source": [
    "# Load data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uknEcA8Wi5vM"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_excel(train_file_name, header=None)\n",
    "train_df.columns = ['Input', 'Label']\n",
    "train_df = train_df.dropna().drop_duplicates().reset_index(drop=True, inplace=False)\n",
    "\n",
    "dev_df = pd.read_excel(dev_file_name, header=None)\n",
    "dev_df.columns = ['Input', 'Label']\n",
    "\n",
    "test_df = pd.read_excel(test_file_name, header=None, engine='openpyxl')\n",
    "test_df.columns = ['Input', 'Label']\n",
    "\n",
    "# Labels mapped to integers\n",
    "train_df['Label'] = train_df.apply(lambda x:  class_list.index(x['Label']),axis=1)\n",
    "dev_df['Label'] = dev_df.apply(lambda x:  class_list.index(x['Label']),axis=1)\n",
    "test_df['Label'] = test_df.apply(lambda x:  class_list.index(x['Label']),axis=1)\n",
    "\n",
    "\n",
    "print(f'Number of exmaples in the train set: {train_df.shape[0]}')\n",
    "print(f'Number of exmaples in the validation set: {dev_df.shape[0]}')\n",
    "print(f'Number of exmaples in the test set: {test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od3sxFORtE0o"
   },
   "source": [
    "# How sample data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xqLQLrCtA_R"
   },
   "outputs": [],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EnZ0Lj-Lz8u"
   },
   "source": [
    "# Class-weighting with inverse of #samples in the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsqPN-SFsxL1"
   },
   "outputs": [],
   "source": [
    "inverse_weights = np.array(train_df['Label'].value_counts().sort_index())\n",
    "weights = np.sum(inverse_weights) / inverse_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9-vuoxVoprb"
   },
   "source": [
    "# Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPYK6TCvlbK0"
   },
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "train_args ={\"reprocess_input_data\": True,\n",
    "             \"overwrite_output_dir\": True,\n",
    "             \"fp16\":False,\n",
    "             \"num_train_epochs\": 10, # run for 10 epochs\n",
    "             \"no_save\": True, # don't save the weights after each iteration as it exceeds the runtime memory allowed\n",
    "             }\n",
    "\n",
    "class_weighting = input('Do you want to use class weighting:\\nPress\\nY for Yes\\nN for No: ')\n",
    "\n",
    "if class_weighting.lower() == 'n':\n",
    "    # Create a Classification Model (WITHOUT USING CLASS-WEIGHTING)\n",
    "    print('Model not using class-weighting')\n",
    "    model = ClassificationModel(\n",
    "        \"distilbert\", \"distilbert-base-multilingual-cased\",\n",
    "        num_labels=len(class_list),\n",
    "        args=train_args\n",
    ")\n",
    "\n",
    "else:\n",
    "    # Create a Classification Model (USING CLASS-WEIGHTING)\n",
    "    print('Model using class-weighting')\n",
    "    model = ClassificationModel(\n",
    "        \"distilbert\", \"distilbert-base-multilingual-cased\",\n",
    "        num_labels=len(class_list),\n",
    "        weight=list(weights),\n",
    "        args=train_args\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEPi3LtPovLR"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9A8ATV2oueD"
   },
   "outputs": [],
   "source": [
    "def to_str(text):\n",
    "    tt = text['Input']\n",
    "    if not isinstance(tt, str):\n",
    "        tt = str(tt)\n",
    "    return pd.Series([tt, text['Label']])\n",
    "\n",
    "model.train_model(train_df.apply(lambda row: to_str(row), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Saychiz-MDbI"
   },
   "source": [
    "# Check performance over test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "im2JOaNqwfTg"
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtOZc62to69z"
   },
   "outputs": [],
   "source": [
    "# precision_recall_fscore_support\n",
    "def scr(labels, preds):\n",
    "    return score(labels, preds, average='macro')\n",
    "\n",
    "# f1 score\n",
    "def f1_multiclass(labels, preds):\n",
    "    return f1_score(labels, preds, average='micro')\n",
    "    \n",
    "# classification report\n",
    "def classification_rprt(labels, preds):\n",
    "    return classification_report(labels, preds, output_dict=True, target_names=class_list)\n",
    "\n",
    "# matthews correlation coefficient \n",
    "def mathews_coff(labels, pred):\n",
    "    return matthews_corrcoef(labels, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71KCuXCkwkdB"
   },
   "source": [
    "## Evaluate over the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5vBgUaqwuaT"
   },
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(dev_df.apply(lambda x: to_str(x), axis=1), f1=f1_multiclass, acc=accuracy_score, \n",
    "                                                            cls_report=classification_rprt, mathews_coff = mathews_coff, score=scr)\n",
    "\n",
    "print(f\"Val acc: {result['acc']}\\nEval_loss: {result['eval_loss']}\\nf1 score: {result['f1']}\\nMatthews corrcoeff: {result['mathews_coff']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYewVBxYMPZZ"
   },
   "source": [
    "## The Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hh74yL36xSv"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(result['cls_report']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3UhonZw2DRI"
   },
   "source": [
    "## The precision, recall and f1 scores over the dev-set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-br13OIM5knR"
   },
   "outputs": [],
   "source": [
    "print(f\"Precision: \\t{result['score'][0]} \\nRecall: \\t{result['score'][1]} \\nF1-score: \\t{result['score'][2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNwUYazZMc_V"
   },
   "source": [
    "# Make a dataframe to write the results to an excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec2gXgPG2iT4"
   },
   "outputs": [],
   "source": [
    "if lang == 'Kannada' or lang == 'Tamil':\n",
    "    result_df = pd.concat([pd.DataFrame({'Input': [test_df.iloc[i]['Input']],\n",
    "                            'Correct Label': [class_list[test_df.iloc[i]['Label']]], \n",
    "                            'Predicted Label': [class_list[np.argmax(model_outputs[i])]],\n",
    "                            'Score_Not_offensive': [model_outputs[i][0]],\n",
    "                            'Score_Offensive_Targeted_Insult_Group': [model_outputs[i][1]],\n",
    "                            'Score_Offensive_Targeted_Insult_Individual': [model_outputs[i][2]],\n",
    "                            'Score_Offensive_Targeted_Insult_Other': [model_outputs[i][3]],\n",
    "                            'Score_Offensive_Untargetede': [model_outputs[i][4]],\n",
    "                            f'Score_not-{lang}': [model_outputs[i][5]]\n",
    "                            \n",
    "                            }) for i, j in enumerate(model_outputs)],  ignore_index=True)\n",
    "    \n",
    "else:\n",
    "    result_df = pd.concat([pd.DataFrame({'Input': [test_df.iloc[i]['Input']],\n",
    "                            'Correct Label': [class_list[test_df.iloc[i]['Label']]], \n",
    "                            'Predicted Label': [class_list[np.argmax(model_outputs[i])]],\n",
    "                            'Score_Not_offensive': [model_outputs[i][0]],\n",
    "                            'Score_Offensive_Targeted_Insult_Group': [model_outputs[i][1]],\n",
    "                            'Score_Offensive_Targeted_Insult_Individual': [model_outputs[i][2]],\n",
    "                            'Score_Offensive_Untargetede': [model_outputs[i][3]],\n",
    "                            f'Score_not-{lang}': [model_outputs[i][4]]\n",
    "                            \n",
    "                            }) for i, j in enumerate(model_outputs)],  ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0U6j59Su5nne"
   },
   "source": [
    "## Normalize the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FytSEJ_X5ti7"
   },
   "outputs": [],
   "source": [
    "# To normalize the scores between 0 and 1 (both inclusive)\n",
    "req_cols = [col for col in result_df.columns if col.startswith('Score')]\n",
    "\n",
    "def normalize(row):\n",
    "    vals = np.array([row[col] for col in req_cols])\n",
    "    req_vals = np.exp(vals)/sum(np.exp(vals))\n",
    "    for i,col in enumerate(req_cols):\n",
    "        row[col] = req_vals[i]\n",
    "\n",
    "    return pd.Series(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3vaWL-B0tU_"
   },
   "source": [
    "# Write results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQKoAk4PlWMF"
   },
   "outputs": [],
   "source": [
    "result_df = result_df.apply(lambda row: normalize(row), axis=1)\n",
    "result_df.to_excel(f'mbert_val_{lang}_without_psefind_probsudo-labelling.xlsx', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DHT-cIZMxxc"
   },
   "source": [
    "# Pseudo-labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNNxBpCw1BYg"
   },
   "source": [
    "### 1) Obtain predictions over test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JG2gN7KUqyjY"
   },
   "outputs": [],
   "source": [
    "test_df['Input'] = test_df['Input'].apply(lambda val: str(val))\n",
    "predictions, raw_outputs = model.predict(test_df['Input'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq7VT_oa1Inf"
   },
   "source": [
    "### 2) Combine the test-set inputs and predictions obtained with the train-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bWTXsKjzlWO"
   },
   "outputs": [],
   "source": [
    "test_df['Label'] = predictions\n",
    "pseudo_label_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKonDUM7NLhS"
   },
   "source": [
    "### 3) Train the model again with pseudo-labelled data included "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNQJlo3Y6kzG"
   },
   "outputs": [],
   "source": [
    "model.train_model(pseudo_label_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIOkd7tw2atd"
   },
   "source": [
    "# Check performance over dev-set with the new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31dAaUXz2jts"
   },
   "source": [
    "### Evaluate over the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjC7GNaD6k2T"
   },
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(dev_df.apply(lambda x: to_str(x), axis=1), f1=f1_multiclass, acc=accuracy_score, \n",
    "                                                            cls_report=classification_rprt, mathews_coff = mathews_coff, score=scr)\n",
    "\n",
    "print(f\"Val acc: {result['acc']}\\nEval_loss: {result['eval_loss']}\\nf1 score: {result['f1']}\\nMatthews corrcoeff: {result['mathews_coff']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGTJCEvl4W0h"
   },
   "source": [
    "## The Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flmU9amH6k54"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(result['cls_report']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FgwgICw4a8M"
   },
   "source": [
    "## The precision, recall and f1 scores over the dev-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nZnu8T74pxQ"
   },
   "outputs": [],
   "source": [
    "print(f\"Precision: {result['score'][0]} \\nRecall: {result['score'][1]} \\nF1-score: {result['score'][2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsTnF-264rM7"
   },
   "source": [
    "# Make a dataframe to write the results to an excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfXjVzDIOR87"
   },
   "outputs": [],
   "source": [
    "if lang == 'Kannada' or lang == 'Tamil':\n",
    "    result_df = pd.concat([pd.DataFrame({'Input': [dev_df.iloc[i]['Input']],\n",
    "                            'Correct Label': [class_list[dev_df.iloc[i]['Label']]], \n",
    "                            'Predicted Label': [class_list[np.argmax(model_outputs[i])]],\n",
    "                            'Score_Not_offensive': [model_outputs[i][0]],\n",
    "                            'Score_Offensive_Targeted_Insult_Group': [model_outputs[i][1]],\n",
    "                            'Score_Offensive_Targeted_Insult_Individual': [model_outputs[i][2]],\n",
    "                            'Score_Offensive_Targeted_Insult_Other': [model_outputs[i][3]],\n",
    "                            'Score_Offensive_Untargetede': [model_outputs[i][4]],\n",
    "                            f'Score_not-{lang}': [model_outputs[i][5]]\n",
    "                            \n",
    "                            }) for i, j in enumerate(model_outputs)],  ignore_index=True)\n",
    "    \n",
    "else:\n",
    "    result_df = pd.concat([pd.DataFrame({'Input': [dev_df.iloc[i]['Input']],\n",
    "                            'Correct Label': [class_list[dev_df.iloc[i]['Label']]], \n",
    "                            'Predicted Label': [class_list[np.argmax(model_outputs[i])]],\n",
    "                            'Score_Not_offensive': [model_outputs[i][0]],\n",
    "                            'Score_Offensive_Targeted_Insult_Group': [model_outputs[i][1]],\n",
    "                            'Score_Offensive_Targeted_Insult_Individual': [model_outputs[i][2]],\n",
    "                            'Score_Offensive_Untargetede': [model_outputs[i][3]],\n",
    "                            f'Score_not-{lang}': [model_outputs[i][4]]\n",
    "                            \n",
    "                            }) for i, j in enumerate(model_outputs)],  ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH1o9I6w6fwi"
   },
   "source": [
    "## Write results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-hzx047OgcR"
   },
   "outputs": [],
   "source": [
    "result_df = result_df.apply(lambda row: normalize(row), axis=1)\n",
    "result_df.to_excel(f'mbert_val_{lang}_with_pseudo-labelling.xlsx', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DistilmBERT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
